{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import copy \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "try:\n",
    "    from nltk import tokenize\n",
    "    use_nltk = True\n",
    "except ImportError:\n",
    "    use_nltk = False\n",
    "\n",
    "OAI = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "ARXIV = '{http://arxiv.org/OAI/arXiv/}'\n",
    "BASE = 'http://export.arxiv.org/oai2?verb=ListRecords&'\n",
    "\n",
    "ABS_URL = \"http://arxiv.org/abs/{:s}\"\n",
    "PDF_URL = \"http://arxiv.org/pdf/{:s}.pdf\"\n",
    "\n",
    "# By default, this only works for astro-ph.\n",
    "CAT = \"physics:astro-ph\"\n",
    "SUBCAT = ['GA', 'CO', 'EP', 'HE', 'IM', 'SR']\n",
    "\n",
    "SEARCH_TYPE = ['today', 'yesterday', 'from_yesterday', 'past_seven', 'user']\n",
    "\n",
    "TODAY = datetime.datetime.today().replace(hour=0, minute=0, second=0)\n",
    "YESTERDAY = (TODAY - datetime.timedelta(days=1)).replace(hour=0, minute=0, second=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_sub_class(papers, sub_cat, no_crosslist=True):\n",
    "    \"\"\"\n",
    "    Filter the search results and keep the ones in certain sub-category.\n",
    "    \"\"\"\n",
    "    if no_crosslist:\n",
    "        # The first sub-category needs to be the desired one\n",
    "        return [\n",
    "            \"astro-ph.{:s}\".format(sub_cat.strip()) == p['sub_cat'].split()[0].strip() \n",
    "            for p in papers]\n",
    "    return [\"astro-ph.{:s}\".format(sub_cat.strip()) in p['sub_cat'] for p in papers]\n",
    "\n",
    "def _get_text(meta, tag):\n",
    "    \"\"\"Extracts text from an xml field\"\"\"\n",
    "    try:\n",
    "        return meta.find(ARXIV + tag).text.strip().replace('\\n', ' ')\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _date_str(date):\n",
    "    \"\"\"\n",
    "    Convert the datetime into a string with '%Y-%m-%d' format.\n",
    "    \"\"\"\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def gather_dates(search_type, date_from=None, date_until=None):\n",
    "    \"\"\"\n",
    "    Get the from and until dates for the search.\n",
    "    \"\"\"\n",
    "    if search_type is None:\n",
    "        search_type = 'user'\n",
    "\n",
    "    search_type = search_type.lower().strip()\n",
    "\n",
    "    if search_type not in SEARCH_TYPE:\n",
    "        raise ValueError(\n",
    "            \"Wrong search type: \", SEARCH_TYPE)\n",
    "\n",
    "    if search_type == 'today':\n",
    "        date_from = TODAY\n",
    "        date_until = TODAY\n",
    "    elif search_type == 'yesterday':\n",
    "        date_from = YESTERDAY\n",
    "        date_until = YESTERDAY\n",
    "    elif search_type == 'from_yesterday':\n",
    "        date_from = YESTERDAY\n",
    "        date_until = TODAY\n",
    "    elif search_type == 'past_seven':\n",
    "        date_from = TODAY - datetime.timedelta(days=7)\n",
    "        date_until = TODAY\n",
    "    else:\n",
    "        if date_from is None:\n",
    "            date_from = TODAY\n",
    "        else:\n",
    "            try:\n",
    "                date_from = datetime.datetime.strptime(date_from, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                print(\"Date format should be: YYYY-MM-DD\")\n",
    "\n",
    "        if date_until is None:\n",
    "            date_until = TODAY\n",
    "        else:\n",
    "            try:\n",
    "                date_until = datetime.datetime.strptime(date_until, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                print(\"Date format should be: YYYY-MM-DD\")\n",
    "\n",
    "    return date_from, date_until\n",
    "\n",
    "\n",
    "def organize_meta(meta):\n",
    "    \"\"\"\n",
    "    Organize the metadata.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': _get_text(meta, 'id'),\n",
    "        'title': _get_text(meta, 'title'),\n",
    "        'abstract': _get_text(meta, 'abstract'),\n",
    "        'sub_cat': _get_text(meta, 'categories'),\n",
    "        'created': datetime.datetime.strptime(_get_text(meta, 'created'), '%Y-%m-%d')\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape(url, sleep_time=30, timeout=300, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the search results.\n",
    "    \"\"\"\n",
    "    t0, elapsed = time.time(), 0\n",
    "    results, batch = [], 1\n",
    "\n",
    "    while True:\n",
    "        if verbose:\n",
    "            print('Fetching up to {:d} records...'.format(1000 * batch))\n",
    "\n",
    "        # Arxiv only allows you to scrape 1000 words at a time.\n",
    "        try:\n",
    "            response = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            if e.code == 503:\n",
    "                _ = int(e.hdrs.get('retry-after', sleep_time))\n",
    "                print('Got 503. Retrying after {0:d} seconds.'.format(sleep_time))\n",
    "                time.sleep(sleep_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        batch += 1\n",
    "\n",
    "        # Get the full XML output\n",
    "        xml_output = response.read()\n",
    "        xml_root = ET.fromstring(xml_output)\n",
    "\n",
    "        # Get all the search records\n",
    "        records = xml_root.findall(OAI + 'ListRecords/' + OAI + 'record')\n",
    "\n",
    "        for record in records:\n",
    "            # Get the metadata of the record\n",
    "            meta = record.find(OAI + 'metadata').find(ARXIV + 'arXiv')\n",
    "            results.append(meta)\n",
    "\n",
    "        try:\n",
    "            token = xml_root.find(OAI + 'ListRecords').find(OAI + 'resumptionToken')\n",
    "        except:\n",
    "            return 1\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "        else:\n",
    "            url = BASE + 'resumptionToken=%s' % token.text\n",
    "\n",
    "        t1 = time.time()\n",
    "        elapsed += (t1 - t0)\n",
    "\n",
    "        if elapsed >= timeout:\n",
    "            break\n",
    "        else:\n",
    "            t0 = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print('Total number of records {:d}'.format(len(results)))\n",
    "\n",
    "    return results\n",
    "\n",
    "def astroph_abstract(output='output.md', search_type='user', date_cushion=2.5,\n",
    "                     date_from=None, date_until=None, sub_cat=None,\n",
    "                     verbose=False, sleep_time=30, timeout=300, no_crosslist=True):\n",
    "    \"\"\"\n",
    "    Gather the abstracts of the astro-ph within a period of time, and output a summary\n",
    "    markdown file.\n",
    "\n",
    "    Based on: https://github.com/Mahdisadjadi/arxivscraper by Mahdisadjadi\n",
    "    \"\"\"\n",
    "    # Get the from and unitl date\n",
    "    date_f, date_u = gather_dates(search_type, date_from=date_from, date_until=date_until)\n",
    "\n",
    "    # Form the search URL\n",
    "    search_url = BASE + 'from={:s}&until={:s}&metadataPrefix=arXiv&set={:s}'.format(\n",
    "        _date_str(date_f).strip(), _date_str(date_u).strip(), CAT)\n",
    "\n",
    "    metadata = scrape(search_url, sleep_time=sleep_time, timeout=timeout, verbose=verbose)\n",
    "\n",
    "    paper_records = [organize_meta(meta) for meta in metadata]\n",
    "\n",
    "    # Remove the recently updated one\n",
    "    # TODO: This is not perfect\n",
    "    # - If someone created a preprint long before the submission, it will be left out\n",
    "    papers = Table(\n",
    "        [p for p in paper_records if p['created'] >= (\n",
    "            date_f - datetime.timedelta(days=date_cushion))])\n",
    "\n",
    "    # Filter the search results through sub-categories\n",
    "    if isinstance(sub_cat, str):\n",
    "        papers_keep = papers[_filter_sub_class(papers, sub_cat, no_crosslist=no_crosslist)]\n",
    "    elif isinstance(sub_cat, list):\n",
    "        papers_keep = papers[np.logical_or.reduce(\n",
    "            [_filter_sub_class(papers, s, no_crosslist=no_crosslist) for s in sub_cat])]\n",
    "    else:\n",
    "        papers_keep = papers\n",
    "\n",
    "    # Organize the results into markdown format (line-by-line)\n",
    "    markdown_list = []\n",
    "\n",
    "    if date_f == date_u:\n",
    "        markdown_list.append(\"### {:s}\".format(_date_str(date_f)))\n",
    "    else:\n",
    "        markdown_list.append(\"### {:s} to {:s}\".format(\n",
    "            _date_str(date_f), _date_str(date_u)))\n",
    "\n",
    "    for p in papers_keep:\n",
    "        abs_url = ABS_URL.format(p['id'])\n",
    "        pdf_url = PDF_URL.format(p['id'])\n",
    "        markdown_list.append(\n",
    "            \"\\n##### [{:s}]({:s}) [(PDF)]({:s})\\n\".format(\n",
    "                ' '.join(p['title'].split()), abs_url, pdf_url))\n",
    "\n",
    "        abstract = p['abstract'].replace('\\\\,', ' ')\n",
    "        abstract.replace('et al.', 'et al')\n",
    "        if not use_nltk:\n",
    "            markdown_list.append(\"- {:s}\".format(' '.join(abstract.split())))\n",
    "        else:\n",
    "            sentences = tokenize.sent_tokenize(' '.join(abstract.split()))\n",
    "            for s in sentences:\n",
    "                markdown_list.append(\"- {:s}\".format(s))\n",
    "\n",
    "    # Write the markdown to file\n",
    "    with open(output, 'w') as f:\n",
    "        for line in markdown_list:\n",
    "            f.write(\"{:s}\\n\".format(line))\n",
    "\n",
    "    return papers_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 1000 records...\n",
      "Total number of records 115\n"
     ]
    }
   ],
   "source": [
    "a = astroph_abstract(\n",
    "    output='output.md', search_type='today', date_from=None, date_until=None, sub_cat='GA', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 1000 records...\n",
      "Total number of records 863\n"
     ]
    }
   ],
   "source": [
    "b = astroph_abstract(\n",
    "    output='output.md', search_type='past_seven', sub_cat='CO', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 1000 records...\n",
      "Total number of records 592\n"
     ]
    }
   ],
   "source": [
    "c = astroph_abstract(\n",
    "    output='output.md', date_from='2021-01-28', date_until='2021-02-03', sub_cat='GA', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 1000 records...\n",
      "Total number of records 115\n"
     ]
    }
   ],
   "source": [
    "d = astroph_abstract(\n",
    "    output='output.md', search_type='today', date_from=None, date_until=None, sub_cat=['GA', 'CO', 'IM'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=43</i>\n",
       "<table id=\"table4945783056\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>id</th><th>title</th><th>abstract</th><th>sub_cat</th><th>created</th></tr></thead>\n",
       "<thead><tr><th>str10</th><th>str175</th><th>str1920</th><th>str54</th><th>object</th></tr></thead>\n",
       "<tr><td>2101.10337</td><td>Extracting Dark-Matter Velocities from Halo Masses: A Reconstruction   Conjecture</td><td>The distribution of primordial dark-matter velocities can significantly influence the growth of cosmological structure. In principle, one can therefore exploit the halo-mass distribution in order to learn about the dark sector. In practice, however, this task is both theoretically and computationally intractable. In this paper, we propose a simple one-line conjecture which can be used to &quot;reconstruct&quot; the primordial dark-matter velocity distribution directly from the shape of the halo-mass function. Although our conjecture is completely heuristic, we show that it successfully reproduces the salient features of the underlying dark-matter velocity distribution -- even for non-trivial distributions which are highly non-thermal and/or multi-modal, such as might occur for non-minimal dark sectors. Our conjecture therefore provides an operational tool for probing the dark sector which does not rely on the existence of non-gravitational couplings between dark and visible states.</td><td>astro-ph.CO hep-ph</td><td>2021-01-25 00:00:00</td></tr>\n",
       "<tr><td>2101.10340</td><td>Extreme-Value Distributions and Primordial Black-Hole Formation</td><td>We argue that primordial black-hole formation must be described by means of extreme-value theory. This is a consequence of the large values of the energy density required to initiate the collapse of black holes in the early Universe and the finite duration of their collapse. Compared to the Gaussian description of the most extreme primordial density fluctuations, the holes&apos; mass function is narrower and peaks towards larger masses. Secondly, thanks to the shallower fall-off of extreme-value distributions, the predicted abundance of primordial black holes is boosted by $10^{7}$ orders of magnitude when extrapolating the observed nearly scale-free power spectrum of the cosmic large-scale structure to primordial black-hole mass scales.</td><td>astro-ph.CO gr-qc hep-ph hep-th math-ph math.MP</td><td>2021-01-25 00:00:00</td></tr>\n",
       "<tr><td>2101.10360</td><td>A convolutional-neural-network estimator of CMB constraints on dark   matter energy injection</td><td>We show that the impact of energy injection by dark matter annihilation on the cosmic microwave background power spectra can be apprehended via a residual likelihood map. By resorting to convolutional neural networks that can fully discover the underlying pattern of the map, we propose a novel way of constraining dark matter annihilation based on the Planck 2018 data. We demonstrate that the trained neural network can efficiently predict the likelihood and accurately place bounds on the annihilation cross-section in a $\\textit{model-independent}$ fashion. The machinery will be made public in the near future.</td><td>astro-ph.CO hep-ph</td><td>2021-01-25 00:00:00</td></tr>\n",
       "<tr><td>2101.10714</td><td>Relieving the $H_0$ tension with a new interacting dark energy model</td><td>We investigate an extended cosmological model motivated by the asymptotic safety of gravitational field theory, in which the matter and radiation densities and the cosmological constant receive a correction parametrized by the parameters $\\delta_G$ and $\\delta_\\Lambda$, leading to that both the evolutions of the matter and radiation densities and the cosmological constant slightly deviate from the standard forms. Here we explain this model as a scenario of vacuum energy interacting with matter and radiation. We consider two cases of the model: (i) ${\\tilde\\Lambda}$CDM with one additional free parameter $\\delta_G$ and (ii) e${\\tilde\\Lambda}$CDM with two additional free parameters $\\delta_G$ and $\\delta_\\Lambda$. We use two data combinations, CMB+BAO+SN (CBS) and CMB+BAO+SN+$H_0$ (CBSH), to constrain the models. We find that, in the case of using the CBS data, neither ${\\tilde\\Lambda}$CDM nor e${\\tilde\\Lambda}$CDM can effectively alleviate the $H_0$ tension. However, it is found that using the CBSH data the $H_0$ tension can be greatly relieved by the models. In particular, in the case of e${\\tilde\\Lambda}$CDM, the $H_0$ tension can be resolved to 0.6$\\sigma$. We find that as an interacting dark energy model, ${\\tilde\\Lambda}$CDM is much better than $\\Lambda(t)$CDM in the sense of both relieving the $H_0$ tension and fitting to the current observational data.</td><td>astro-ph.CO gr-qc hep-ph</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11014</td><td>The cosmology dependence of galaxy clustering and lensing from a hybrid   $N$-body-perturbation theory model</td><td>We implement a model for the two-point statistics of biased tracers that combines dark matter dynamics from $N$-body simulations with an analytic Lagrangian bias expansion. Using Aemulus, a suite of $N$-body simulations built for emulation of cosmological observables, we emulate the cosmology dependence of these nonlinear spectra from redshifts $z = 0$ to $z=2$. We quantify the accuracy of our emulation procedure, which is sub-per cent at $k=1\\, h {\\rm Mpc}^{-1}$ for the redshifts probed by upcoming surveys and improves at higher redshifts. We demonstrate its ability to describe the statistics of complex tracer samples, including those with assembly bias and baryonic effects, reliably fitting the clustering and lensing statistics of such samples at redshift $z\\simeq 0.4$ to scales of $k_{\\rm max} \\approx 0.6\\, h\\mathrm{Mpc}^{-1}$. We show that the emulator can be used for unbiased cosmological parameter inference in simulated joint clustering and galaxy--galaxy lensing analyses with data drawn from an independent $N$-body simulation. These results indicate that our emulator is a promising tool that can be readily applied to the analysis of current and upcoming datasets from galaxy surveys.</td><td>astro-ph.CO astro-ph.IM</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11016</td><td>Explaining Excess Dipole in NVSS Data Using Superhorizon Perturbation</td><td>Many observations in recent times have shown evidence against the standard assumption of isotropy in the Big Bang model. Introducing a superhorizon scalar metric perturbation has been able to explain some of these anomalies. In this work, we probe the net velocity arising due to the perturbation, which does not cancel out for large scale structure, unlike in the case of CMB. Thus, within this model&apos;s framework, our velocity with respect to the CMB is different from the velocity with respect to the large scale structure. Taking this extra velocity component into account, we study the superhorizon mode&apos;s implications for the excess dipole observed in the NRAO VLA Sky Survey (NVSS). We find that the mode can consistently explain both the CMB and NVSS observations. We also find that the model is consistent with the observed Hubble constant dipole and the Hubble bulk flow velocity. The model leads to several predictions which can be tested in future surveys. In particular, it implies that the observed dipole in large scale structure should be redshift dependent and should show an increase in amplitude with redshift. We also find that the Hubble parameter should show a dipole anisotropy whose amplitude must increase with redshift in the CMB frame. Similar anisotropic behaviour is expected for the observed redshift as a function of the luminosity distance.</td><td>astro-ph.CO</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11088</td><td>CosmoReionMC: A package for estimating cosmological and astrophysical   parameters using CMB, Lyman-{\\alpha} absorption and global 21 cm data</td><td>We present a Markov Chain Monte Carlo (MCMC)-based parameter estimation package, CosmoReionMC, to jointly constrain cosmological parameters of the $\\Lambda$CDM model and the astrophysical parameters related to hydrogen reionization. The package is based on a previously developed physically motivated semi-analytical model for reionization, a similar semi-analytical model for computing the global 21~cm signal during the cosmic dawn and using an appropriately modified version of the publicly available CAMB for computing the CMB anisotropies. These calculations are then coupled to an MCMC ensemble sampler \\texttt{emcee} to compute the posterior distributions of the model parameter. The model has twelve free parameters in total: five cosmological and seven related to the stellar populations. We constrain the parameters by matching the theoretical predictions with CMB data from Planck, observations related to the quasar absorption spectra and, for the first time, the global 21~cm signal from EDGES. We find that incorporating the quasar spectra data in the analysis tightens the bounds on the electron scattering optical depth $\\tau$ and consequently the normalization $A_s$ of the primordial matter power spectrum (or equivalently $\\sigma_8$). Furthermore, when we include the EDGES data in the analysis, we find that an early population of metal-free stars with efficient radio emission is necessary to match the absorption amplitude. The CosmoReionMC package should have interesting future applications, e.g., probing non-standard extensions to the $\\Lambda$CDM model.</td><td>astro-ph.CO</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11098</td><td>A possible mass distribution of primordial black holes implied by   LIGO-Virgo</td><td>The LIGO-Virgo Collaboration has so far detected around 90 black holes, some of which have masses larger than what were expected from the collapse of stars. The mass distribution of LIGO-Virgo black holes appears to have a peak at $\\sim30M_{\\odot}$ and two tails on the ends. By assuming that they all have a primordial origin, we analyze the GWTC-1 (O1\\&amp;O2) and GWTC-2 (O3a) datasets by performing maximum likelihood estimation on a broken power law mass function $f(m)$, with the result $f\\propto m^{1.2}$ for $m&lt;35M_{\\odot}$ and $f\\propto m^{-4}$ for $m&gt;35M_{\\odot}$. This appears to behave better than the popular log-normal mass function. Surprisingly, such a simple and unique distribution can be realized in our previously proposed mechanism of PBH formation, where the black holes are formed by vacuum bubbles that nucleate during inflation via quantum tunneling. Moreover, this mass distribution can also provide an explanation to supermassive black holes formed at high redshifts.</td><td>astro-ph.CO gr-qc hep-th</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11181</td><td>A Generative Model of Galactic Dust Emission Using Variational Inference</td><td>Emission from the interstellar medium can be a significant contaminant of measurements of the intensity and polarization of the cosmic microwave background (CMB). For planning CMB observations, and for optimizing foreground-cleaning algorithms, a description of the statistical properties of such emission can be helpful. Here we examine a machine learning approach to inferring the statistical properties of dust from either observational data or physics-based simulations. In particular, we apply a type of neural network called a Variational Auto Encoder (VAE) to maps of the intensity of emission from interstellar dust as inferred from Planck sky maps and demonstrate its ability to a) simulate new samples with similar summary statistics as the training set, b) provide fits to emission maps withheld from the training set, and c) produce constrained realizations. We find VAEs are easier to train than another popular architecture: that of Generative Adversarial Networks (GANs), and are better-suited for use in Bayesian inference.</td><td>astro-ph.CO astro-ph.GA</td><td>2021-01-26 00:00:00</td></tr>\n",
       "<tr><td>2101.11244</td><td>Gravitational waves from type II axion-like curvaton model and its   implication for NANOGrav result</td><td>The recent report of NANOGrav is gathering attention since its signal can be explained by the stochastic gravitational waves (GWs) with $\\Omega_{\\rm GW}\\sim 10^{-9}$ at $f\\sim 10^{-8}$Hz. The PBH formation scenario is one of the candidates for the NANOGrav signal, which can simultaneously explain the observed $30 M_\\odot$ black holes in the binary merger events in LIGO-Virgo collaboration. We focus on the type II axion-like curvaton model of the PBH formation. In type II model the complex field whose phase part is the axion rolls down from the origin of the potential. It is found that type II model achieves the broad power spectrum of the density perturbations and can simultaneously explain the LIGO-Virgo events and the NANOGrav signal. We also improve the treatment of the non-Gaussianity of perturbations in our model to estimate the amplitude of the induced GWs precisely.</td><td>astro-ph.CO hep-ph</td><td>2021-01-27 00:00:00</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>2102.01030</td><td>Inflation: a quantum laboratory on cosmological scales</td><td>This thesis is dedicated to studying cosmological inflation, which is a period of accelerated expansion in the very early Universe that is required to explain the observed anisotropies in the cosmic microwave background. Inflation, when combined with quantum mechanics, also provides the over-densities that grow into the structure of the modern Universe. Understanding perturbations during this period of inflation is important, and we study these perturbations in detail in this work. We will assume that inflation is driven by a single scalar field, called the inflaton. When the shape of the potential energy is flat, the inflaton can enter a phase of &quot;ultra-slow-roll inflation&quot;. We study the stability of such a period of inflation, and find that it can be stable and long-lived, although it has a dependence on the initial velocity of the inflaton field. This is different to the slow-roll regime of inflation, which is always stable, but has no dependence on the initial velocity. In the second part of this thesis, we use the stochastic formalism for inflation in order to take account of the non-perturbative backreaction of quantum fluctuations during inflation. We use this formalism to study curvature fluctuations during inflation, and we derive full probability distributions of these fluctuations. This allows us to study the statistics of large fluctuations that can lead to the formation of rare objects, such as primordial black holes. In general, we find that when the quantum effects modelled by the stochastic formalism are correctly accounted for, many more primordial black holes can be formed than one would expect if these quantum effects were not taken into account.</td><td>astro-ph.CO gr-qc hep-th</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01031</td><td>CosmoLattice</td><td>This is the user manual for CosmoLattice, a modern package for lattice simulations of the dynamics of interacting scalar and gauge fields in an expanding universe. CosmoLattice incorporates a series of features that makes it very versatile and powerful: $i)$ it is written in C++ fully exploiting the object oriented programming paradigm, with a modular structure and a clear separation between the physics and the technical details, $ii)$ it is MPI-based and uses a discrete Fourier transform parallelized in multiple spatial dimensions, which makes it specially appropriate for probing scenarios with well-separated scales, running very high resolution simulations, or simply very long ones, $iii)$ it introduces its own symbolic language, defining field variables and operations over them, so that one can introduce differential equations and operators in a manner as close as possible to the continuum, $iv)$ it includes a library of numerical algorithms, ranging from $O(\\delta t^2)$ to $O(\\delta t^{10})$ methods, suitable for simulating global and gauge theories in an expanding grid, including the case of `self-consistent&apos; expansion sourced by the fields themselves. Relevant observables are provided for each algorithm (e.g.~energy densities, field spectra, lattice snapshots) and we note that remarkably all our algorithms for gauge theories always respect the Gauss constraint to machine precision. In this manual we explain how to obtain and run CosmoLattice in a computer (let it be your laptop, desktop or a cluster). We introduce the general structure of the code and describe in detail the basic files that any user needs to handle. We explain how to implement any model characterized by a scalar potential and a set of scalar fields, either singlets or interacting with $U(1)$ and/or $SU(2)$ gauge fields. CosmoLattice is publicly available at www.cosmolattice.net.</td><td>astro-ph.CO gr-qc hep-lat hep-ph</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01045</td><td>Delensing the CMB with the cosmic infrared background: the impact of   foregrounds</td><td>The most promising avenue for detecting primordial gravitational waves from cosmic inflation is through measurements of degree-scale CMB $B$-mode polarisation. This approach must face the challenge posed by gravitational lensing of the CMB, which obscures the signal of interest. Fortunately, the lensing effects can be partially removed by combining high-resolution $E$-mode measurements with an estimate of the projected matter distribution. For near-future experiments, the best estimate of the latter will arise from co-adding internal reconstructions (derived from the CMB itself) with external tracers of the large-scale structure such as the cosmic infrared background (CIB). In this work, we characterise how foregrounds impact the delensing procedure when CIB intensity, $I$, is used as the matter tracer. We find that higher-point functions of the CIB and Galactic dust such as $\\langle BEI \\rangle_{c}$ and $\\langle EIEI \\rangle_{c}$ can, in principle, bias the power spectrum of delensed $B$-modes. After estimating the dust residuals in currently-available CIB maps and upcoming, foreground-cleaned Simons Observatory CMB data, we find, using non-Gaussian dust simulations, that the bias to any primordial signal is small compared to statistical errors for ground-based experiments, but might be significant for space-based experiments probing very large angular scales. However, mitigation techniques based on multi-frequency cleaning appear to be very effective. We also show, by means of an analytic model, that the bias arising from the higher-point functions of the CIB itself ought to be negligible.</td><td>astro-ph.CO</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01068</td><td>Constraining the Baryon Abundance with the Kinematic Sunyaev-Zel&apos;dovich   Effect: Projected-Field Detection Using Planck, WMAP, and unWISE</td><td>The kinematic Sunyaev-Zel&apos;dovich (kSZ) effect -- the Doppler boosting of cosmic microwave background (CMB) photons scattering off free electrons with non-zero line-of-sight velocity -- is an excellent probe of the distribution of baryons in the Universe. In this paper, we measure the kSZ effect due to ionized gas traced by infrared-selected galaxies from the \\emph{unWISE} catalog. We employ the &quot;projected-field&quot; kSZ estimator, which does not require spectroscopic galaxy redshifts. To suppress non-kSZ foreground signals associated with the galaxies (e.g., dust emission and thermal SZ), this estimator requires cleaned CMB maps, which we obtain from \\emph{Planck} and \\emph{WMAP} data. Using a new &quot;asymmetric&quot; estimator that combines different foreground-cleaned CMB maps to maximize the signal-to-noise, we measure the kSZ$^2$-galaxy cross-power spectrum for three subsamples of the \\emph{unWISE} galaxy catalog, which peak at mean redshifts $z \\approx$ 0.6, 1.1, and 1.5, have average halo mass $\\sim 1$-$5\\times 10^{13}$ $h^{-1} M_{\\odot}$, and in total contain over 500 million galaxies. After marginalizing over CMB lensing contributions, we measure the amplitude of the kSZ signal $A_{\\rm kSZ^2} = 0.42 \\pm 0.31$, $5.02 \\pm 1.01$, and $8.23 \\pm 3.23$, for the three subsamples, where $A_{\\rm kSZ^2} = 1$ corresponds to our fiducial model. The combined kSZ detection S/N $&gt;$ 5. We discuss possible explanations for the excess kSZ signal associated with the $z \\approx 1.1$ sample, and show that foreground contamination in the CMB maps is very unlikely to be the cause. Our measurements illustrate clearly that no baryons are missing on large scales at low redshifts.</td><td>astro-ph.CO</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01096</td><td>Properties of clumps and filaments around galaxy clusters</td><td>We report on the possibility of studying the properties of cosmic diffuse baryons by studying self-gravitating clumps and filaments connected to galaxy clusters. While filaments are challenging to detect with X-ray observations, the higher density of clumps make them visible and a viable tracer to study the thermodynamical properties of baryons undergoing accretion along cosmic web filaments onto galaxy clusters. We developed new algorithms to identify these structures in a set of non-radiative high-resolution simulations of galaxy clusters, cosmological simulations of galaxy clusters. We show that the density and temperature of clumps are independent of the mass of the cluster where they reside. We detected a positive correlation between the filament temperature and the host cluster mass. Density and Temperature of clumps and filaments tend to correlate. Both decrease moving outward. We observe that clumps are hotter, more massive and more luminous if identified closer to the cluster center. Clumps and filaments contribute to ~17 (1) per cent of the gas mass (volume) outside R500,c, with clumps contributing a factor of 2 more. Especially in the outermost cluster regions (~3*R500,c or beyond) X-ray observations might already have the chance of locating filaments based on the distribution of clumps, and by studying the thermodynamics of diffuse baryons before they are processed by the dynamical interaction with the host intracluster medium.</td><td>astro-ph.CO</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01184</td><td>Cosmological cross-correlations and nearest neighbor distributions</td><td>Cross-correlations between datasets are used in many different contexts in cosmological analyses. Recently, $k$-Nearest Neighbor Cumulative Distribution Functions ($k{\\rm NN}$-${\\rm CDF}$) were shown to be sensitive probes of cosmological (auto) clustering. In this paper, we extend the framework of nearest neighbor measurements to describe joint distributions of, and correlations between, two datasets. We describe the measurement of joint $k{\\rm NN}$-${\\rm CDF}$s, and show that these measurements are sensitive to all possible connected $N$-point functions that can be defined in terms of the two datasets. We describe how the cross-correlations can be isolated by combining measurements of the joint $k{\\rm NN}$-${\\rm CDF}$s and those measured from individual datasets. We demonstrate the application of these measurements in the context of Gaussian density fields, as well as for fully nonlinear cosmological datasets. Using a Fisher analysis, we show that measurements of the halo-matter cross-correlations, as measured through nearest neighbor measurements are more sensitive to the underlying cosmological parameters, compared to traditional two-point cross-correlation measurements over the same range of scales. Finally, we demonstrate how the nearest neighbor cross-correlations can robustly detect cross correlations between sparse samples -- the same regime where the two-point cross-correlation measurements are dominated by noise.</td><td>astro-ph.CO</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01292</td><td>Using a multi-messenger and multi-wavelength observational strategy to   probe the nature of dark energy through direct measurements of cosmic   expansion history</td><td>In the forthcoming decades, the redshift drift observations in optical and radio bands will provide accurate measurements on $H(z)$ covering the redshift ranges of $2&lt;z&lt;5$ and $0&lt;z&lt;1$. In addition, gravitational wave (GW) standard siren observations could make measurements on the dipole anisotropy of luminosity distance, which will also provide the $H(z)$ measurements in the redshift range of $0&lt;z&lt;3$. In this work, we propose a multi-messenger and multi-wavelength observational strategy to measure $H(z)$ based on the three next-generation projects, E-ELT, SKA, and DECIGO, and we wish to see whether the future $H(z)$ measurements could provide tight constraints on dark-energy parameters. It is found that E-ELT, SKA1, and DECIGO are highly complementary in constraining dark energy models using the $H(z)$ data. We find that E-ELT, SKA1, and DECIGO can tightly constrain $\\Omega_m$, $w$ (or $w_0$), and $H_0$, respectively, and thus the combination of them could effectively break the cosmological parameter degeneracies. The joint E-ELT+SKA1+DECIGO data give $\\sigma(w)\\approx 0.02$ in the $w$CDM model and $\\sigma(w_0)\\approx 0.03$ in the CPL model, which are better than the results of {\\it Planck} 2018 TT,TE,EE+lowE+lensing+SNe+BAO. But even the joint data cannot well constrain $w_a$ in the CPL model.</td><td>astro-ph.CO gr-qc hep-ph</td><td>2021-02-01 00:00:00</td></tr>\n",
       "<tr><td>2102.01365</td><td>Cosmological Parameter Estimation from the Two-Dimensional Genus   Topology -- Measuring the Expansion History using the Genus Amplitude as a   Standard Ruler</td><td>We measure the genus of the galaxy distribution in two-dimensional slices of the SDSS-III BOSS catalog to constrain the cosmological parameters governing the expansion history of the Universe. The BOSS catalogs are divided into twelve concentric shells over the redshift range $0.25 &lt; z &lt; 0.6$ and we repeatedly measure the genus from the two-dimensional galaxy density fields, each time varying the cosmological parameters used to infer the distance-redshift relation to the shells. We also indirectly reconstruct the two-dimensional genus amplitude using the three-dimensional genus measured from SDSS Main Galaxy Sample with galaxies at low redshift $z &lt; 0.12$. We combine the low- and high-redshift measurements, finding the cosmological model which minimizes the redshift evolution of the genus amplitude, using the fact that this quantity should be conserved. Being a distance measure, the test is sensitive to the matter density parameter ($\\Omega_{\\rm m}$) and equation of state of dark energy ($w_{\\rm de}$). We find a constraint of $w_{\\rm de} = -1.05^{+0.13}_{-0.12}$, $\\Omega_{\\rm m} = 0.303 \\pm 0.036$ after combining the high- and low-redshift measurements and combining with Planck CMB data. Higher redshift data and combining data sets at low redshift will allow for stronger constraints.</td><td>astro-ph.CO</td><td>2021-02-02 00:00:00</td></tr>\n",
       "<tr><td>2102.01398</td><td>Magnetic field generation from bubble collisions during first-order   phase transition</td><td>We study the magnetic fields generation from the cosmological first-order electroweak phase transition. We calculate the magnetic field induced by the variation of the Higgs phase for two bubbles and three bubbles collisions. Our study shows that electromagnetic currents in the collision direction produce the ring-like magnetic field in the intersect regions of colliding bubbles, which may seed the primordial magnetic field that are constrained by intergalatic field observations.</td><td>astro-ph.CO hep-ph hep-th</td><td>2021-02-02 00:00:00</td></tr>\n",
       "<tr><td>2102.01637</td><td>Precision cosmology made more precise</td><td>So far, the standard attitude to solve the Friedmann equations in the simultaneous presence of radiation $R$, matter $M$ and cosmological constant ${\\Lambda}$ is to find solutions $R_R (t)$, $R_M (t)$ and $R_{\\Lambda} (t)$ separately for each individual component alone, and next to join them together, thereby obtaining a piecewise solution $R_{\\rm pw} (t)$. We instead find the exact and analytic solution $R (t)$ of the same equations in flat space. Moreover, we quantify the error made when $R_{\\rm pw} (t)$ is used in place of $R (t)$.</td><td>astro-ph.CO</td><td>2021-02-02 00:00:00</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=43>\n",
       "    id     ...       created      \n",
       "  str10    ...        object      \n",
       "---------- ... -------------------\n",
       "2101.10337 ... 2021-01-25 00:00:00\n",
       "2101.10340 ... 2021-01-25 00:00:00\n",
       "2101.10360 ... 2021-01-25 00:00:00\n",
       "2101.10714 ... 2021-01-26 00:00:00\n",
       "2101.11014 ... 2021-01-26 00:00:00\n",
       "2101.11016 ... 2021-01-26 00:00:00\n",
       "2101.11088 ... 2021-01-26 00:00:00\n",
       "2101.11098 ... 2021-01-26 00:00:00\n",
       "2101.11181 ... 2021-01-26 00:00:00\n",
       "2101.11244 ... 2021-01-27 00:00:00\n",
       "       ... ...                 ...\n",
       "2102.01030 ... 2021-02-01 00:00:00\n",
       "2102.01031 ... 2021-02-01 00:00:00\n",
       "2102.01045 ... 2021-02-01 00:00:00\n",
       "2102.01068 ... 2021-02-01 00:00:00\n",
       "2102.01096 ... 2021-02-01 00:00:00\n",
       "2102.01184 ... 2021-02-01 00:00:00\n",
       "2102.01292 ... 2021-02-01 00:00:00\n",
       "2102.01365 ... 2021-02-02 00:00:00\n",
       "2102.01398 ... 2021-02-02 00:00:00\n",
       "2102.01637 ... 2021-02-02 00:00:00"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cat = c[1]['sub_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astro-ph.GA']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_cat.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'astro-ph.GA' == c[1]['sub_cat'].split()[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
