{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import copy \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox exploring the arXiv OAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "try:\n",
    "    from nltk import tokenize\n",
    "    use_nltk = True\n",
    "except ImportError:\n",
    "    use_nltk = False\n",
    "\n",
    "OAI = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "ARXIV = '{http://arxiv.org/OAI/arXiv/}'\n",
    "BASE = 'http://export.arxiv.org/oai2?verb=ListRecords&'\n",
    "\n",
    "ABS_URL = \"http://arxiv.org/abs/{:s}\"\n",
    "PDF_URL = \"http://arxiv.org/pdf/{:s}.pdf\"\n",
    "\n",
    "# By default, this only works for astro-ph.\n",
    "CAT = \"physics:astro-ph\"\n",
    "SUBCAT = ['GA', 'CO', 'EP', 'HE', 'IM', 'SR']\n",
    "\n",
    "SEARCH_TYPE = ['today', 'yesterday', 'from_yesterday', 'past_seven', 'user']\n",
    "\n",
    "TODAY = datetime.datetime.today().replace(hour=0, minute=0, second=0)\n",
    "YESTERDAY = (TODAY - datetime.timedelta(days=1)).replace(hour=0, minute=0, second=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_sub_class(papers, sub_cat, no_crosslist=True):\n",
    "    \"\"\"\n",
    "    Filter the search results and keep the ones in certain sub-category.\n",
    "    \"\"\"\n",
    "    if no_crosslist:\n",
    "        # The first sub-category needs to be the desired one\n",
    "        return [\n",
    "            \"astro-ph.{:s}\".format(sub_cat.strip()) == p['sub_cat'].split()[0].strip() \n",
    "            for p in papers]\n",
    "    return [\"astro-ph.{:s}\".format(sub_cat.strip()) in p['sub_cat'] for p in papers]\n",
    "\n",
    "def _get_text(meta, tag):\n",
    "    \"\"\"Extracts text from an xml field\"\"\"\n",
    "    try:\n",
    "        return meta.find(ARXIV + tag).text.strip().replace('\\n', ' ')\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _date_str(date):\n",
    "    \"\"\"\n",
    "    Convert the datetime into a string with '%Y-%m-%d' format.\n",
    "    \"\"\"\n",
    "    return date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def gather_dates(search_type, date_from=None, date_until=None):\n",
    "    \"\"\"\n",
    "    Get the from and until dates for the search.\n",
    "    \"\"\"\n",
    "    if search_type is None:\n",
    "        search_type = 'user'\n",
    "\n",
    "    search_type = search_type.lower().strip()\n",
    "\n",
    "    if search_type not in SEARCH_TYPE:\n",
    "        raise ValueError(\n",
    "            \"Wrong search type: \", SEARCH_TYPE)\n",
    "\n",
    "    if search_type == 'today':\n",
    "        date_from = TODAY\n",
    "        date_until = TODAY\n",
    "    elif search_type == 'yesterday':\n",
    "        date_from = YESTERDAY\n",
    "        date_until = YESTERDAY\n",
    "    elif search_type == 'from_yesterday':\n",
    "        date_from = YESTERDAY\n",
    "        date_until = TODAY\n",
    "    elif search_type == 'past_seven':\n",
    "        date_from = TODAY - datetime.timedelta(days=7)\n",
    "        date_until = TODAY\n",
    "    else:\n",
    "        if date_from is None:\n",
    "            date_from = TODAY\n",
    "        else:\n",
    "            try:\n",
    "                date_from = datetime.datetime.strptime(date_from, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                print(\"Date format should be: YYYY-MM-DD\")\n",
    "\n",
    "        if date_until is None:\n",
    "            date_until = TODAY\n",
    "        else:\n",
    "            try:\n",
    "                date_until = datetime.datetime.strptime(date_until, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                print(\"Date format should be: YYYY-MM-DD\")\n",
    "\n",
    "    return date_from, date_until\n",
    "\n",
    "\n",
    "def organize_meta(meta):\n",
    "    \"\"\"\n",
    "    Organize the metadata.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': _get_text(meta, 'id'),\n",
    "        'title': _get_text(meta, 'title'),\n",
    "        'abstract': _get_text(meta, 'abstract'),\n",
    "        'sub_cat': _get_text(meta, 'categories'),\n",
    "        'created': datetime.datetime.strptime(_get_text(meta, 'created'), '%Y-%m-%d')\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape(url, sleep_time=30, timeout=300, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the search results.\n",
    "    \"\"\"\n",
    "    t0, elapsed = time.time(), 0\n",
    "    results, batch = [], 1\n",
    "\n",
    "    while True:\n",
    "        if verbose:\n",
    "            print('Fetching up to {:d} records...'.format(1000 * batch))\n",
    "\n",
    "        # Arxiv only allows you to scrape 1000 words at a time.\n",
    "        try:\n",
    "            response = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            if e.code == 503:\n",
    "                _ = int(e.hdrs.get('retry-after', sleep_time))\n",
    "                print('Got 503. Retrying after {0:d} seconds.'.format(sleep_time))\n",
    "                time.sleep(sleep_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        batch += 1\n",
    "\n",
    "        # Get the full XML output\n",
    "        xml_output = response.read()\n",
    "        xml_root = ET.fromstring(xml_output)\n",
    "\n",
    "        # Get all the search records\n",
    "        records = xml_root.findall(OAI + 'ListRecords/' + OAI + 'record')\n",
    "\n",
    "        for record in records:\n",
    "            # Get the metadata of the record\n",
    "            meta = record.find(OAI + 'metadata').find(ARXIV + 'arXiv')\n",
    "            results.append(meta)\n",
    "\n",
    "        try:\n",
    "            token = xml_root.find(OAI + 'ListRecords').find(OAI + 'resumptionToken')\n",
    "        except:\n",
    "            return 1\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "        else:\n",
    "            url = BASE + 'resumptionToken=%s' % token.text\n",
    "\n",
    "        t1 = time.time()\n",
    "        elapsed += (t1 - t0)\n",
    "\n",
    "        if elapsed >= timeout:\n",
    "            break\n",
    "        else:\n",
    "            t0 = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print('Total number of records {:d}'.format(len(results)))\n",
    "\n",
    "    return results\n",
    "\n",
    "def astroph_abstract(output='output.md', search_type='user', date_cushion=2.5,\n",
    "                     date_from=None, date_until=None, sub_cat=None,\n",
    "                     verbose=False, sleep_time=30, timeout=300, no_crosslist=True):\n",
    "    \"\"\"\n",
    "    Gather the abstracts of the astro-ph within a period of time, and output a summary\n",
    "    markdown file.\n",
    "\n",
    "    Based on: https://github.com/Mahdisadjadi/arxivscraper by Mahdisadjadi\n",
    "    \"\"\"\n",
    "    # Get the from and unitl date\n",
    "    date_f, date_u = gather_dates(search_type, date_from=date_from, date_until=date_until)\n",
    "\n",
    "    # Form the search URL\n",
    "    search_url = BASE + 'from={:s}&until={:s}&metadataPrefix=arXiv&set={:s}'.format(\n",
    "        _date_str(date_f).strip(), _date_str(date_u).strip(), CAT)\n",
    "\n",
    "    metadata = scrape(search_url, sleep_time=sleep_time, timeout=timeout, verbose=verbose)\n",
    "\n",
    "    paper_records = [organize_meta(meta) for meta in metadata]\n",
    "\n",
    "    # Remove the recently updated one\n",
    "    # TODO: This is not perfect\n",
    "    # - If someone created a preprint long before the submission, it will be left out\n",
    "    papers = Table(\n",
    "        [p for p in paper_records if p['created'] >= (\n",
    "            date_f - datetime.timedelta(days=date_cushion))])\n",
    "\n",
    "    # Filter the search results through sub-categories\n",
    "    if isinstance(sub_cat, str):\n",
    "        papers_keep = papers[_filter_sub_class(papers, sub_cat, no_crosslist=no_crosslist)]\n",
    "    elif isinstance(sub_cat, list):\n",
    "        papers_keep = papers[np.logical_or.reduce(\n",
    "            [_filter_sub_class(papers, s, no_crosslist=no_crosslist) for s in sub_cat])]\n",
    "    else:\n",
    "        papers_keep = papers\n",
    "\n",
    "    # Organize the results into markdown format (line-by-line)\n",
    "    markdown_list = []\n",
    "\n",
    "    if date_f == date_u:\n",
    "        markdown_list.append(\"### {:s}\".format(_date_str(date_f)))\n",
    "    else:\n",
    "        markdown_list.append(\"### {:s} to {:s}\".format(\n",
    "            _date_str(date_f), _date_str(date_u)))\n",
    "\n",
    "    for p in papers_keep:\n",
    "        abs_url = ABS_URL.format(p['id'])\n",
    "        pdf_url = PDF_URL.format(p['id'])\n",
    "        markdown_list.append(\n",
    "            \"\\n##### [{:s}]({:s}) [(PDF)]({:s})\\n\".format(\n",
    "                ' '.join(p['title'].split()), abs_url, pdf_url))\n",
    "\n",
    "        abstract = p['abstract'].replace('\\\\,', ' ')\n",
    "        abstract.replace('et al.', 'et al')\n",
    "        if not use_nltk:\n",
    "            markdown_list.append(\"- {:s}\".format(' '.join(abstract.split())))\n",
    "        else:\n",
    "            sentences = tokenize.sent_tokenize(' '.join(abstract.split()))\n",
    "            for s in sentences:\n",
    "                markdown_list.append(\"- {:s}\".format(s))\n",
    "\n",
    "    # Write the markdown to file\n",
    "    with open(output, 'w') as f:\n",
    "        for line in markdown_list:\n",
    "            f.write(\"{:s}\\n\".format(line))\n",
    "\n",
    "    return papers_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astroph_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 1000 records...\n",
      "Total number of records 336\n",
      "Will only keep items from sub-category: [G A]\n",
      "Will exclude cross-listed items\n",
      "Keep 15 preprints\n"
     ]
    }
   ],
   "source": [
    "a = astroph_abstract.astroph_abstract(\n",
    "    output='output.md', search_type='today', date_from=None, date_until=None, sub_cat='GA', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present analysis of the spatial density structure for the outer disk from 8$-$14 \\,kpc with the LAMOST DR5 13534 OB-type stars. We discover the clear and similar flaring signatures beyond 8 \\,kpc for which the scale height is from 0.14 to 0.5 \\,kpc in the north and south side, implying that the flaring is possibly symmetrical in the Milky Way disk. We reveal that the thickness of the OB stellar disk is similar with our previous thin disk traced by red giant branch stars, possibly implying that secular evolution is not the main contributor to the flaring but other scenarios such as interactions with passing dwarf galaxies should be more possible. When comparing the OB stellar disk with the gas disk, the former one is moderately thicker than the later one, meaning that one could tentatively use young OB-type stars to trace the gas properties. Meanwhile, we unravel that the scale length of the young OB stellar disk is 1.17 $\\pm$ 0.05 \\,kpc, which is shorter than gas disk, comfirming that the gas disk is more extended than stellar disk. What is more, by considering the mid-plane displacements ($Z_{0}$) in our density model we find that almost all are within 100 \\,pc with the increasing trend as Galactocentric distance increases, which looks like the warp signal in young stellar population but the scenarios will need to be investigated in more detail.\n"
     ]
    }
   ],
   "source": [
    "print(a[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
